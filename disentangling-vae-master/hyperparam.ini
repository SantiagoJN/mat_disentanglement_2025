[Paths]
    results_dir = "/path/to/folder" # Enter here the desired folder where results will be stored
    runs_root = "/path/to/folder"
[Datasets]
    training_dataset = "masked-serrano-train" 
    test_dataset = "serrano"
    use_labels = False
    img_size = (3, 256, 256) # Make sure that the channel size matches the type of samples we're using!
[Testing]
    recon_loss_mode = 1 # For more information about the modes, check the implementation in losses.py
    num_samples = 32 # Number of samples for the loss_mode=2
    random_samples = False # Whether we want to get random samples every time we run the testing or not (useful for getting visualizations with same data)
    # If the above is false, down here we will introduce the specific indices we want to use
    idcs = [0,1,2,3,4,5,6,7,8,9,10,11,12,0,1,2,3,4,5,6,7,8,9,10,11,12,0,1,2,3,4,5,6,7,8,9,10,11,12,0,1,2,3,4,5,6,7,8,9,10,11,12]
    get_representations = False
    num_representations = 100
    evaluation_path = "/path/to/folder"
    save_dataset = False # To save the .npy of the specific dataset
[Custom]
    # General options
    log_level = "info"
    no_progress_bar = False
    no_cuda = False
    use_normals = False
    use_lab = None
    seed = 1234
    frequency = 0

    # Training options
    epochs = 4001 # increased a little bit for the optuna TPE pruner..
    batch_size = 64
    lr = 5e-4
    checkpoint_every = 30
    dataset = 'mnist'
    experiment = 'custom'
    skip = 2
    ratio = 0
    drop = 0

    # Model Options
    model = 'Burgess'
    loss = "betaB"
    latent_dim = 10
    rec_dist = "bernoulli"
    # reg_anneal doesn't seem to make much difference but some people say it can help
    reg_anneal = 10000
    from_checkpoint = "Undefined"

    # betaH Options
    betaH_B = 4

    # betaB Options
    betaB_initC = 0
    betaB_finC = 25
    # use 100 which is used by most implementation online
    betaB_G = 1000

    # factor Options
    factor_G = 6
    factor_B = 1
    factor_C = 0
    lr_disc = 5e-5
    warmup_C_epochs = -1
    max_C = 0
    warmup_epochs = -1
    max_beta = 1
    cycle = False
    recon_factor = 1.0
    big = False
    kl_func = 0
    growth_scale = 0.000001
    triplets_name = "Undefined"

    # btcvae Options
    btcvae_A = 1
    btcvae_G = 1
    btcvae_B = 6

    # Evaluations Options
    is_metrics = False
    no_test = False
    is_eval_only = False
    eval_batchsize = 1000

    # * For GECO implementation
    use_GECO = False
    max_dim = 40
    max_error = 55000

# ### DATASET COMMON ###
# same number of epochs for comparaisons

[Common_dsprites]
    dataset = 'dsprites'
    checkpoint_every = 10
    epochs = 30
[Common_chairs]
    dataset = 'chairs'
    checkpoint_every = 100
    epochs = 300
[Common_celeba]
    dataset = 'celeba'
    checkpoint_every = 100
    epochs = 200
[Common_mnist]
    dataset = 'mnist'
    checkpoint_every = 100
    epochs = 400
[Common_fashion]
    dataset = 'fashion'
    checkpoint_every = 100
    epochs = 400

# ### LOSS COMMON ###

[Common_VAE]
    loss = "VAE"
    lr = 5e-4
[Common_betaH]
    loss = "betaH"
    lr = 5e-4
[Common_betaB]
    loss = "betaB"
    lr = 1e-3
    reg_anneal = 100000
[Common_factor]
    loss = "factor"
    lr = 1e-4
[Common_btcvae]
    loss = "btcvae"
    lr = 5e-4

# ### EXPERIMENT SPECIFIC ###
# additional hyperparameter changes besides the common ones

# BETA H

[betaH_dsprites]
# beta as in paper
    betaH_B = 4
[betaH_celeba]
# beta value as in from https://github.com/1Konny/Beta-VAE
    betaH_B = 10
[betaH_chairs]
# beta value as in from https://github.com/1Konny/Beta-VAE
    betaH_B = 4

# BETA B

[betaB_dsprites]
# capacity as in paper
    betaB_finC = 25
[betaB_celeba]
# capacity as in paper
    betaB_finC = 50
[betaB_chairs]
    betaB_finC = 25

# FACTOR

[factor_chairs]
    factor_G = 3.2
    lr_disc = 1e-5
# beta value as in from https://github.com/1Konny/FactorVAE/blob/master/utils.py
[factor_dsprites]
    factor_G = 6.4
    lr_disc = 1e-4
[factor_celeba]
    factor_G = 6.4
    lr_disc = 1e-5

# BTCVAE
# use all same values as factor
[btcvae_chairs]
    btcvae_B = ${factor_chairs:factor_G}
[btcvae_dsprites]
    btcvae_B = ${factor_dsprites:factor_G}
[btcvae_celeba]
    btcvae_B = ${factor_celeba:factor_G}

# Other
# those don't use the common section by default (need to be <loss>_<data> to use)!

[best_celeba]
    btcvae_A = -10
    btcvae_B = 20
    dataset = 'celeba'
    loss = "btcvae"
    epochs = ${Common_celeba:epochs}
    checkpoint_every = ${Common_celeba:checkpoint_every}
    lr = ${Common_btcvae:lr}
    rec_dist = "laplace"

[best_dsprites]
    btcvae_A = -5
    btcvae_B = 10
    dataset = 'dsprites'
    loss = "btcvae"
    epochs = ${Common_dsprites:epochs}
    checkpoint_every = ${Common_dsprites:checkpoint_every}
    lr = ${Common_btcvae:lr}

[debug]
    epochs = 1
    log_level = "debug"
    no_test = True
    reg_anneal = 0
